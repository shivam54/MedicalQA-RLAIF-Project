{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b6b03438ae6d4620a41e92725e718715":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a0f70f5665745b389211f1c2d3ace75","IPY_MODEL_89755b61672046b0859552301b4e2508","IPY_MODEL_68d559a686a14cad9844aa7f2e00419c"],"layout":"IPY_MODEL_1ffc0169c49046f58baf397565917c21"}},"3a0f70f5665745b389211f1c2d3ace75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1ef7fc3241644469b511c5d17cb8a30","placeholder":"​","style":"IPY_MODEL_106658e411724fabad9879a7dd4d439f","value":"Loading checkpoint shards: 100%"}},"89755b61672046b0859552301b4e2508":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_881e959350b4442fb5b7e57bee1a70b8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a606ae486baf479d88021f1f4d569838","value":2}},"68d559a686a14cad9844aa7f2e00419c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37e89ad3911c4e7bbba9919ead7cd36c","placeholder":"​","style":"IPY_MODEL_41b016d04e7948bab5036178336cb5ba","value":" 2/2 [00:00&lt;00:00,  3.11it/s]"}},"1ffc0169c49046f58baf397565917c21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1ef7fc3241644469b511c5d17cb8a30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"106658e411724fabad9879a7dd4d439f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"881e959350b4442fb5b7e57bee1a70b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a606ae486baf479d88021f1f4d569838":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37e89ad3911c4e7bbba9919ead7cd36c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b016d04e7948bab5036178336cb5ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":328085,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":275325,"modelId":296218}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch datasets transformers peft optimum auto-gptq tqdm\n\nimport torch\nimport requests\nimport json\nimport re\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\nfrom datasets import load_dataset, Dataset\nfrom tqdm import tqdm\nimport os","metadata":{"id":"Unowr9wd7BfP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1832fd30-3f53-454e-8574-cba30a7354c5","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:57:21.292982Z","iopub.execute_input":"2025-04-08T16:57:21.293195Z","iopub.status.idle":"2025-04-08T16:57:49.787423Z","shell.execute_reply.started":"2025-04-08T16:57:21.293173Z","shell.execute_reply":"2025-04-08T16:57:49.786712Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting optimum\n  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\nCollecting auto-gptq\n  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq)\n  Downloading gekko-1.3.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading optimum-1.24.0-py3-none-any.whl (433 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading gekko-1.3.0-py3-none-any.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0mm\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge, gekko, optimum, auto-gptq\nSuccessfully installed auto-gptq-0.7.1 gekko-1.3.0 optimum-1.24.0 rouge-1.0.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"login(token=\"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\")","metadata":{"id":"JGY5KaRk7eaH","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:58:29.943968Z","iopub.execute_input":"2025-04-08T16:58:29.944308Z","iopub.status.idle":"2025-04-08T16:58:30.212161Z","shell.execute_reply.started":"2025-04-08T16:58:29.944283Z","shell.execute_reply":"2025-04-08T16:58:30.211513Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Device setup\ndevice = torch.device(\"cuda\")","metadata":{"id":"moCJbkSx7i5z","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:58:32.762885Z","iopub.execute_input":"2025-04-08T16:58:32.763179Z","iopub.status.idle":"2025-04-08T16:58:32.766717Z","shell.execute_reply.started":"2025-04-08T16:58:32.763156Z","shell.execute_reply":"2025-04-08T16:58:32.765847Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load fine-tuned Qwen model\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbase_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\nfine_tuned_model = PeftModel.from_pretrained(base_model, \"/kaggle/input/qwen_finetuned_lora_quantized/pytorch/default/1/qwen_finetuned_lora_quantized/final\").to(device)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"","metadata":{"id":"Ke_y7xVQ7mJI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d861fcb-697f-404e-e67d-5ea72e1197d6","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:59:32.893939Z","iopub.execute_input":"2025-04-08T16:59:32.894248Z","iopub.status.idle":"2025-04-08T16:59:35.664476Z","shell.execute_reply.started":"2025-04-08T16:59:32.894221Z","shell.execute_reply":"2025-04-08T16:59:35.663535Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load evaluator model\nevaluator_name = \"Qwen/Qwen2.5-3B-Instruct\"\neval_tokenizer = AutoTokenizer.from_pretrained(evaluator_name)\neval_model = AutoModelForCausalLM.from_pretrained(evaluator_name, torch_dtype=torch.bfloat16).to(device)\nif eval_tokenizer.pad_token is None:\n    eval_tokenizer.pad_token = eval_tokenizer.eos_token\neval_tokenizer.padding_side = \"left\"","metadata":{"id":"aTotwCcP7p32","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b6b03438ae6d4620a41e92725e718715","3a0f70f5665745b389211f1c2d3ace75","89755b61672046b0859552301b4e2508","68d559a686a14cad9844aa7f2e00419c","1ffc0169c49046f58baf397565917c21","d1ef7fc3241644469b511c5d17cb8a30","106658e411724fabad9879a7dd4d439f","881e959350b4442fb5b7e57bee1a70b8","a606ae486baf479d88021f1f4d569838","37e89ad3911c4e7bbba9919ead7cd36c","41b016d04e7948bab5036178336cb5ba"]},"outputId":"633138fd-96d7-4382-d137-91acea70e9b3","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:00:39.384138Z","iopub.execute_input":"2025-04-08T17:00:39.384543Z","iopub.status.idle":"2025-04-08T17:01:05.324982Z","shell.execute_reply.started":"2025-04-08T17:00:39.384512Z","shell.execute_reply":"2025-04-08T17:01:05.324231Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df298f7e38f64bbfb2885320bf795016"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eebc0fdad4b481790deed205fcb6a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d90435dba0d43d481c508b0b58e28b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417eb26f3e784c87978652f62b4b6633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c069a502ce4a4492c4dc742468b4d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c5b985ab96e41d58c5bd18ff5a47fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa10028129f4b2ebd6a0307cb834b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9f25ce1dd541688e7c5fd566171399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da25af1ca3148768596b3e35b47ed08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"336cbe8eaad445a3808b8c4b47b3d223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de4dca20326940abb7c60147e0d1bb84"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Load full dataset\ndef load_medical_dataset():\n    dataset = load_dataset(\"TsinghuaC3I/UltraMedical-Preference\", split=\"train\")\n    return dataset","metadata":{"id":"nMaBgfpN7sOF","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:01:14.007869Z","iopub.execute_input":"2025-04-08T17:01:14.008191Z","iopub.status.idle":"2025-04-08T17:01:14.012277Z","shell.execute_reply.started":"2025-04-08T17:01:14.008163Z","shell.execute_reply":"2025-04-08T17:01:14.011500Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Extract question and correct answer\ndef extract_question_answer(row):\n    question = row[\"prompt\"]\n    correct_answer = next(entry[\"content\"] for entry in row[\"chosen\"] if entry[\"role\"] == \"assistant\")\n    return question, correct_answer","metadata":{"id":"EP9yqbaK7wfE","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:01:14.645597Z","iopub.execute_input":"2025-04-08T17:01:14.645919Z","iopub.status.idle":"2025-04-08T17:01:14.650095Z","shell.execute_reply.started":"2025-04-08T17:01:14.645891Z","shell.execute_reply":"2025-04-08T17:01:14.649088Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Generate answers from fine-tuned model\ndef generate_answers_finetuned(question):\n    prompt_messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": question}\n    ]\n    prompt_text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True).to(device)\n    attention_mask = inputs.attention_mask\n    answers = []\n    for _ in range(2):\n        with torch.no_grad():\n            output = fine_tuned_model.generate(\n                inputs.input_ids,\n                attention_mask=attention_mask,\n                max_new_tokens=512,\n                do_sample=True,\n                top_p=0.95,\n                temperature=0.7\n            )\n        full_response = tokenizer.decode(output[0], skip_special_tokens=True)\n        assistant_response = full_response.split(\"assistant\")[-1].strip() if \"assistant\" in full_response else full_response.strip()\n        answers.append(assistant_response)\n    return answers","metadata":{"id":"it8TifPe7zbZ","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:01:19.580734Z","iopub.execute_input":"2025-04-08T17:01:19.581023Z","iopub.status.idle":"2025-04-08T17:01:19.586703Z","shell.execute_reply.started":"2025-04-08T17:01:19.581000Z","shell.execute_reply":"2025-04-08T17:01:19.585778Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def rank_answers(question, correct_answer, generated_answers):\n    prompt = (\n        f\"Question: {question}\\n\"\n        f\"Correct Answer: {correct_answer}\\n\\n\" +\n        \"For each of the following generated answers, assign a score between 0 and 1 (where 1 is perfectly correct)\" +\n        \"based on how well it matches the correct answer. Consider factual accuracy, completeness, and key points.\" +\n        \"Even partially correct answers should receive a score above 0. Use decimals (e.g., 0.3, 0.7) as needed.\\n\\n\"\n        f\"Generated Answer 0: {generated_answers[0]}\\n\\n\"\n        f\"Generated Answer 1: {generated_answers[1]}\\n\\n\"\n        \"Output only the scores for each generated answer in this exact format:\\n\"\n        \"Generated Answer 0 score: \\n\"\n        \"Generated Answer 1 score: \\n\"\n    )\n    inputs = eval_tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n    attention_mask = inputs.attention_mask\n    with torch.no_grad():\n        outputs = eval_model.generate(\n            inputs.input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=1000,\n            do_sample=True,\n            top_p=0.95,\n            temperature=0.8\n        )\n    return eval_tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"id":"3q16B11673M-","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:01:20.581085Z","iopub.execute_input":"2025-04-08T17:01:20.581423Z","iopub.status.idle":"2025-04-08T17:01:23.667587Z","shell.execute_reply.started":"2025-04-08T17:01:20.581391Z","shell.execute_reply":"2025-04-08T17:01:23.666676Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Parse evaluator output safely\ndef safe_parse_evaluator_output(eval_output):\n    score_dict = {}\n    pattern = r\"Generated Answer (\\d+) score:\\s*([0-9]*\\.?[0-9]+)\"\n    matches = re.findall(pattern, eval_output)\n    for idx, score_str in matches:\n        try:\n            score_dict[int(idx)] = float(score_str)\n        except ValueError:\n            continue\n    return score_dict","metadata":{"id":"wBVuAfrU75TR","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:01:24.760242Z","iopub.execute_input":"2025-04-08T17:01:24.760606Z","iopub.status.idle":"2025-04-08T17:01:24.765007Z","shell.execute_reply.started":"2025-04-08T17:01:24.760577Z","shell.execute_reply":"2025-04-08T17:01:24.764012Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def evaluate_finetuned_model():\n\n    dataset = load_medical_dataset()\n    print(f\"Loaded dataset with {len(dataset)} rows\")\n    \n    start_row = 0\n    end_row = min(250, len(dataset))\n    max_rows = end_row - start_row\n\n    # Lists to store results\n    questions = []\n    correct_answers = []\n    generated_answers_list = []\n    scores_list = []\n\n    for i in tqdm(range(start_row, end_row), desc=\"Evaluating rows\"):\n        row = dataset[i]\n        question, correct_answer = extract_question_answer(row)\n\n        generated_answers = generate_answers_finetuned(question)\n\n        eval_output = rank_answers(question, correct_answer, generated_answers)\n        scores = safe_parse_evaluator_output(eval_output)\n\n        questions.append(question)\n        correct_answers.append(correct_answer)\n        generated_answers_list.append(generated_answers)\n        scores_list.append(scores)\n\n    evaluation_dataset = Dataset.from_dict({\n        \"question\": questions,\n        \"correct_answer\": correct_answers,\n        \"generated_answers\": generated_answers_list,\n        \"scores\": scores_list\n    })\n    return evaluation_dataset","metadata":{"id":"XlmW8VsT77fU","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:45:31.074178Z","iopub.execute_input":"2025-04-08T17:45:31.074559Z","iopub.status.idle":"2025-04-08T17:45:31.080525Z","shell.execute_reply.started":"2025-04-08T17:45:31.074532Z","shell.execute_reply":"2025-04-08T17:45:31.079712Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"evaluation_dataset = evaluate_finetuned_model()","metadata":{"id":"zms4ouU47999","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e965cd9-f6b9-48ea-e838-71600b363601","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T17:45:38.249881Z","iopub.execute_input":"2025-04-08T17:45:38.250175Z"}},"outputs":[{"name":"stdout","text":"Loaded dataset with 109353 rows\n","output_type":"stream"},{"name":"stderr","text":"Evaluating rows:   0%|          | 0/250 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Save dataset locally\noutput_dir = \"/kaggle/working/dataset\"\nos.makedirs(output_dir, exist_ok=True)\nevaluation_dataset.save_to_disk(output_dir)\nprint(f\"Evaluation dataset saved to {output_dir}\")","metadata":{"id":"cz1PeMhB9U3n","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"7xA5NTdImcud"},"outputs":[],"execution_count":null}]}